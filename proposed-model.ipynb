{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "MAX_SEQ_LEN = 128\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "# ====================== Custom Tokenizer ======================\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts, min_freq=1):\n",
    "        self.word2idx = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "        self.idx2word = {0:\"<PAD>\", 1:\"<UNK>\"}\n",
    "        self.min_freq = min_freq\n",
    "        self.build_vocab(texts)\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            words = self.tokenize(text)\n",
    "            for w in words:\n",
    "                word_freq[w] = word_freq.get(w, 0) + 1\n",
    "\n",
    "        idx = 2\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= self.min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx +=1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # simple word tokenizer: lowercase + split on non-alpha\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return words\n",
    "\n",
    "    def encode(self, text, max_len=MAX_SEQ_LEN):\n",
    "        tokens = self.tokenize(text)\n",
    "        token_ids = []\n",
    "        for t in tokens:\n",
    "            token_ids.append(self.word2idx.get(t, self.word2idx[\"<UNK>\"]))\n",
    "        # pad or truncate\n",
    "        if len(token_ids) < max_len:\n",
    "            token_ids = token_ids + [self.word2idx[\"<PAD>\"]] * (max_len - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:max_len]\n",
    "        return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, image_folder, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        text = str(row[\"Captions\"])\n",
    "        label = LABEL_MAP[str(row[\"Label_Sentiment\"]).strip().lower()]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        input_ids = self.tokenizer.encode(text)\n",
    "\n",
    "        return image, input_ids, label\n",
    "\n",
    "# ====================== Custom CNN Backbone ======================\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # 640x640 -> 640x640\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 640x640 -> 320x320\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 320x320 -> 160x160\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 160x160 -> 80x80\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 80x80 -> 40x40\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))  # output: (batch, 512, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten (batch, 512)\n",
    "        return x\n",
    "\n",
    "# ====================== LSTM Text Encoder ======================\n",
    "class LSTMTextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, num_layers=1):\n",
    "        super(LSTMTextEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)  # (batch, seq_len, embedding_dim)\n",
    "        packed_output, (hidden, cell) = self.lstm(embedded)  \n",
    "        # hidden shape: (num_layers * num_directions, batch, hidden_dim)\n",
    "\n",
    "        # Concatenate forward and backward hidden states from last layer\n",
    "        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # (batch, hidden_dim*2)\n",
    "        hidden_cat = self.dropout(hidden_cat)\n",
    "        return hidden_cat\n",
    "\n",
    "# ====================== Combined Model ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes):\n",
    "        super(VisionTextClassifier, self).__init__()\n",
    "        self.vision_model = CustomCNN()\n",
    "        self.text_model = LSTMTextEncoder(vocab_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(512 + HIDDEN_DIM*2, 256)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, image, input_ids):\n",
    "        vision_feat = self.vision_model(image)  # (batch, 512)\n",
    "        text_feat = self.text_model(input_ids)  # (batch, hidden_dim*2)\n",
    "\n",
    "        combined = torch.cat((vision_feat, text_feat), dim=1)\n",
    "        x = self.fc1(combined)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# ====================== Transforms ======================\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((640, 640)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ====================== Prepare Data ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "\n",
    "# Build vocab from captions\n",
    "all_texts = df['Captions'].astype(str).tolist()\n",
    "tokenizer = SimpleTokenizer(all_texts, min_freq=1)\n",
    "vocab_size = len(tokenizer.word2idx)\n",
    "\n",
    "dataset = VisionTextDataset(df, tokenizer=tokenizer, image_folder=\"Dataset/Memes\", transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ====================== Train & Evaluate ======================\n",
    "model = VisionTextClassifier(vocab_size=vocab_size, num_classes=len(LABEL_MAP)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, val_losses, val_accuracies, val_f1s = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\")\n",
    "    for image, input_ids, label in loop:\n",
    "        image = image.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image, input_ids)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds, labels = [], []\n",
    "    loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\")\n",
    "    with torch.no_grad():\n",
    "        for image, input_ids, label in loop:\n",
    "            image = image.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            outputs = model(image, input_ids)\n",
    "            loss = criterion(outputs, label)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            pred = outputs.argmax(dim=1).cpu().tolist()\n",
    "            preds.extend(pred)\n",
    "            labels.extend(label.cpu().tolist())\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_acc = accuracy_score(labels, preds)\n",
    "    val_f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    val_accuracies.append(val_acc)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}, Val F1={val_f1:.4f}\")\n",
    "\n",
    "# ====================== Plotting ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "conf_mat = confusion_matrix(labels, preds)\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(len(INV_LABEL_MAP))],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(len(INV_LABEL_MAP))])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\", linewidth=2)\n",
    "plt.plot(val_losses, label=\"Validation Loss\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/loss_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\", marker=\"o\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/accuracy_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(val_f1s, label=\"Validation F1 Score\", marker=\"o\", color=\"green\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Validation F1 Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/f1_score_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f494ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "# ====================== Tokenizer ======================\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts, min_freq=1):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.build_vocab(texts, min_freq)\n",
    "\n",
    "    def build_vocab(self, texts, min_freq):\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for word in self.tokenize(text):\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        idx = 2\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    def encode(self, text, max_len=MAX_SEQ_LEN):\n",
    "        tokens = self.tokenize(text)\n",
    "        ids = [self.word2idx.get(t, 1) for t in tokens]\n",
    "        # pad (left) or truncate\n",
    "        ids = ids[-max_len:] if len(ids) > max_len else [0]*(max_len - len(ids)) + ids\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform: image = self.transform(image)\n",
    "        input_ids = self.tokenizer.encode(str(row[\"Captions\"]))\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, input_ids, label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.LeakyReLU(), nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x).flatten(1)\n",
    "\n",
    "# ====================== LSTM Encoder ======================\n",
    "class LSTMTextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=EMBEDDING_DIM, hid_dim=HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.dropout(self.embedding(input_ids))\n",
    "        _, (hidden, _) = self.lstm(emb)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # BiLSTM last hidden state\n",
    "        return hidden\n",
    "\n",
    "# ====================== Final Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vision_model = CustomCNN()\n",
    "        self.text_model = LSTMTextEncoder(vocab_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 + HIDDEN_DIM*2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids):\n",
    "        vis_feat = self.vision_model(image)\n",
    "        txt_feat = self.text_model(input_ids)\n",
    "        x = torch.cat([vis_feat, txt_feat], dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Transforms ======================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# ====================== Load Data ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "texts = df[\"Captions\"].astype(str).tolist()\n",
    "tokenizer = SimpleTokenizer(texts)\n",
    "vocab_size = len(tokenizer.word2idx)\n",
    "\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\", transform=train_transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "train_set.dataset.transform = train_transform\n",
    "val_set.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================== Train ======================\n",
    "model = VisionTextClassifier(vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\")\n",
    "    for img, ids, lbl in loop:\n",
    "        img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img, ids)\n",
    "        loss = criterion(out, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets, val_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for img, ids, lbl in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "            img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "            out = model(img, ids)\n",
    "            val_loss += criterion(out, lbl).item()\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "    val_accs.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={val_losses[-1]:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "# ====================== Plot ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "\n",
    "# ====================== Load GloVe ======================\n",
    "def load_glove_embeddings(glove_path, word2idx, embed_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(v) for v in values[1:]], dtype=torch.float)\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    embedding_matrix = torch.randn(len(word2idx), embed_dim) * 0.05  # random init\n",
    "    embedding_matrix[0] = torch.zeros(embed_dim)  # PAD idx\n",
    "\n",
    "    for word, idx in word2idx.items():\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "    return embedding_matrix\n",
    "\n",
    "# Path to GloVe embeddings\n",
    "glove_path = \"glove/glove.6B.100d.txt\"\n",
    "\n",
    "# ====================== Tokenizer ======================\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts, min_freq=1):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.build_vocab(texts, min_freq)\n",
    "\n",
    "    def build_vocab(self, texts, min_freq):\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for word in self.tokenize(text):\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        idx = 2\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    def encode(self, text, max_len=MAX_SEQ_LEN):\n",
    "        tokens = self.tokenize(text)\n",
    "        ids = [self.word2idx.get(t, 1) for t in tokens]\n",
    "        # pad (left) or truncate\n",
    "        ids = ids[-max_len:] if len(ids) > max_len else [0]*(max_len - len(ids)) + ids\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform: image = self.transform(image)\n",
    "        input_ids = self.tokenizer.encode(str(row[\"Captions\"]))\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, input_ids, label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.LeakyReLU(), nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x).flatten(1)\n",
    "\n",
    "# ====================== LSTM Encoder ======================\n",
    "class LSTMTextEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hid_dim=HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        num_embeddings, emb_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.dropout(self.embedding(input_ids))\n",
    "        _, (hidden, _) = self.lstm(emb)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # BiLSTM last hidden state\n",
    "        return hidden\n",
    "\n",
    "\n",
    "# ====================== Final Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "        self.vision_model = CustomCNN()\n",
    "        self.text_model = LSTMTextEncoder(embedding_matrix)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 + HIDDEN_DIM*2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids):\n",
    "        vis_feat = self.vision_model(image)\n",
    "        txt_feat = self.text_model(input_ids)\n",
    "        x = torch.cat([vis_feat, txt_feat], dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Transforms ======================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# ====================== Load Data ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "texts = df[\"Captions\"].astype(str).tolist()\n",
    "tokenizer = SimpleTokenizer(texts)\n",
    "vocab_size = len(tokenizer.word2idx)\n",
    "\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\", transform=train_transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "train_set.dataset.transform = train_transform\n",
    "val_set.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================== Train ======================\n",
    "embedding_matrix = load_glove_embeddings(glove_path, tokenizer.word2idx, EMBEDDING_DIM)\n",
    "model = VisionTextClassifier(embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\")\n",
    "    for img, ids, lbl in loop:\n",
    "        img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img, ids)\n",
    "        loss = criterion(out, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets, val_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for img, ids, lbl in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "            img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "            out = model(img, ids)\n",
    "            val_loss += criterion(out, lbl).item()\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "    val_accs.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={val_losses[-1]:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "# ====================== Plot ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "\n",
    "# ====================== Load GloVe ======================\n",
    "def load_glove_embeddings(glove_path, word2idx, embed_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(v) for v in values[1:]], dtype=torch.float)\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    embedding_matrix = torch.randn(len(word2idx), embed_dim) * 0.05  # random init\n",
    "    embedding_matrix[0] = torch.zeros(embed_dim)  # PAD idx\n",
    "\n",
    "    for word, idx in word2idx.items():\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "    return embedding_matrix\n",
    "\n",
    "# Path to GloVe embeddings\n",
    "glove_path = \"glove/glove.6B.100d.txt\"\n",
    "\n",
    "# ====================== Tokenizer ======================\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts, min_freq=1):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.build_vocab(texts, min_freq)\n",
    "\n",
    "    def build_vocab(self, texts, min_freq):\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for word in self.tokenize(text):\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        idx = 2\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    def encode(self, text, max_len=MAX_SEQ_LEN):\n",
    "        tokens = self.tokenize(text)\n",
    "        ids = [self.word2idx.get(t, 1) for t in tokens]\n",
    "        # pad (left) or truncate\n",
    "        ids = ids[-max_len:] if len(ids) > max_len else [0]*(max_len - len(ids)) + ids\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform: image = self.transform(image)\n",
    "        input_ids = self.tokenizer.encode(str(row[\"Captions\"]))\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, input_ids, label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.LeakyReLU(), nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x).flatten(1)\n",
    "\n",
    "# ====================== LSTM Encoder ======================\n",
    "class LSTMTextEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hid_dim=HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        num_embeddings, emb_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.dropout(self.embedding(input_ids))\n",
    "        _, (hidden, _) = self.lstm(emb)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # BiLSTM last hidden state\n",
    "        return hidden\n",
    "\n",
    "\n",
    "# ====================== Final Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "        self.vision_model = CustomCNN()\n",
    "        self.text_model = LSTMTextEncoder(embedding_matrix)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 + HIDDEN_DIM*2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids):\n",
    "        vis_feat = self.vision_model(image)\n",
    "        txt_feat = self.text_model(input_ids)\n",
    "        x = torch.cat([vis_feat, txt_feat], dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Transforms ======================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# ====================== Load Data ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "texts = df[\"Captions\"].astype(str).tolist()\n",
    "tokenizer = SimpleTokenizer(texts)\n",
    "vocab_size = len(tokenizer.word2idx)\n",
    "\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\", transform=train_transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "train_set.dataset.transform = train_transform\n",
    "val_set.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================== Train ======================\n",
    "embedding_matrix = load_glove_embeddings(glove_path, tokenizer.word2idx, EMBEDDING_DIM)\n",
    "model = VisionTextClassifier(embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\")\n",
    "    for img, ids, lbl in loop:\n",
    "        img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img, ids)\n",
    "        loss = criterion(out, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets, val_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for img, ids, lbl in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "            img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "            out = model(img, ids)\n",
    "            val_loss += criterion(out, lbl).item()\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "    val_accs.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={val_losses[-1]:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "# ====================== Plot ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "# ====================== Load GloVe ======================\n",
    "def load_glove_embeddings(glove_path, word2idx, embed_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(v) for v in values[1:]], dtype=torch.float)\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    embedding_matrix = torch.randn(len(word2idx), embed_dim) * 0.05\n",
    "    embedding_matrix[0] = torch.zeros(embed_dim)\n",
    "\n",
    "    for word, idx in word2idx.items():\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "    return embedding_matrix\n",
    "\n",
    "glove_path = \"glove/glove.6B.100d.txt\"\n",
    "\n",
    "# ====================== Tokenizer ======================\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts, min_freq=1):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.build_vocab(texts, min_freq)\n",
    "\n",
    "    def build_vocab(self, texts, min_freq):\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for word in self.tokenize(text):\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        idx = 2\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    def encode(self, text, max_len=MAX_SEQ_LEN):\n",
    "        tokens = self.tokenize(text)\n",
    "        ids = [self.word2idx.get(t, 1) for t in tokens]\n",
    "        ids = ids[-max_len:] if len(ids) > max_len else [0]*(max_len - len(ids)) + ids\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform: image = self.transform(image)\n",
    "        input_ids = self.tokenizer.encode(str(row[\"Captions\"]))\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, input_ids, label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x).flatten(1)\n",
    "\n",
    "# ====================== LSTM Encoder ======================\n",
    "class LSTMTextEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "        num_embeddings, emb_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, HIDDEN_DIM, batch_first=True, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.dropout(self.embedding(input_ids))\n",
    "        output, (hn, cn) = self.lstm(emb)\n",
    "        feat = torch.cat((hn[-2], hn[-1]), dim=1)  # BiLSTM\n",
    "        return feat\n",
    "\n",
    "# ====================== Final Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "        self.vision_model = CustomCNN()\n",
    "        self.text_model = LSTMTextEncoder(embedding_matrix)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 + HIDDEN_DIM*2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids):\n",
    "        vis_feat = self.vision_model(image)\n",
    "        txt_feat = self.text_model(input_ids)\n",
    "        x = torch.cat([vis_feat, txt_feat], dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Transforms ======================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# ====================== Load Data ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "texts = df[\"Captions\"].astype(str).tolist()\n",
    "tokenizer = SimpleTokenizer(texts)\n",
    "embedding_matrix = load_glove_embeddings(glove_path, tokenizer.word2idx, EMBEDDING_DIM)\n",
    "\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\", transform=train_transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "train_set.dataset.transform = train_transform\n",
    "val_set.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================== Train ======================\n",
    "model = VisionTextClassifier(embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\")\n",
    "    for img, ids, lbl in loop:\n",
    "        img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img, ids)\n",
    "        loss = criterion(out, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets, val_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for img, ids, lbl in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "            img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "            out = model(img, ids)\n",
    "            val_loss += criterion(out, lbl).item()\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "    val_accs.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={val_losses[-1]:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "# ====================== Plot ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62daf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====================== Configuration ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 100  # Maximum sequence length for text\n",
    "VOCAB_SIZE = 10000  # Vocabulary size for text\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "NUM_CLASSES = 3  # negative, positive, neutral\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "# ====================== Basic Tokenizer ======================\n",
    "def simple_tokenizer(text, vocab, max_len=MAX_LEN):\n",
    "    tokens = text.lower().split()\n",
    "    token_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "    if len(token_ids) < max_len:\n",
    "        token_ids += [vocab[\"<pad>\"]] * (max_len - len(token_ids))\n",
    "    else:\n",
    "        token_ids = token_ids[:max_len]\n",
    "    return token_ids\n",
    "\n",
    "# ====================== Vocabulary Builder ======================\n",
    "def build_vocab(texts, vocab_size=VOCAB_SIZE):\n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = text.lower().split()\n",
    "        counter.update(tokens)\n",
    "    most_common = counter.most_common(vocab_size - 2)\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    vocab.update({word: i + 2 for i, (word, _) in enumerate(most_common)})\n",
    "    return vocab\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, vocab, image_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.label_map = {\"negative\": 0, \"positive\": 1, \"neutral\": 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, row[\"image_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        text = row[\"Captions\"]\n",
    "        text_ids = simple_tokenizer(text, self.vocab)\n",
    "\n",
    "        label = self.label_map[row[\"Label_Sentiment\"]]\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text_ids\": torch.tensor(text_ids, dtype=torch.long),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# ====================== Models ======================\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 112x112\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 56x56\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Linear(128, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.vision_model = CustomCNN()\n",
    "        self.text_model = CustomLSTM(vocab_size, embed_dim, hidden_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, image, text_ids):\n",
    "        vision_feat = self.vision_model(image)\n",
    "        text_feat = self.text_model(text_ids)\n",
    "        combined = torch.cat((vision_feat, text_feat), dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# ====================== Load Dataset ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "vocab = build_vocab(df[\"Captions\"].tolist())\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"Label_Sentiment\"])\n",
    "train_dataset = VisionTextDataset(train_df, vocab, \"Dataset/Memes\", transform)\n",
    "val_dataset = VisionTextDataset(val_df, vocab, \"Dataset/Memes\", transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================== Train ======================\n",
    "model = MultimodalClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        texts = batch[\"text_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f} Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "# ====================== Evaluation ======================\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        texts = batch[\"text_ids\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(images, texts)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# ====================== Report ======================\n",
    "labels_map = [\"negative\", \"positive\", \"neutral\"]\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=labels_map))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=labels_map, yticklabels=labels_map)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e87fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "# ====================== Load GloVe ======================\n",
    "def load_glove_embeddings(glove_path, word2idx, embed_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(v) for v in values[1:]], dtype=torch.float)\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    embedding_matrix = torch.randn(len(word2idx), embed_dim) * 0.05\n",
    "    embedding_matrix[0] = torch.zeros(embed_dim)\n",
    "\n",
    "    for word, idx in word2idx.items():\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "    return embedding_matrix\n",
    "\n",
    "glove_path = \"glove/glove.6B.100d.txt\"\n",
    "\n",
    "# ====================== Tokenizer ======================\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts, min_freq=1):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.build_vocab(texts, min_freq)\n",
    "\n",
    "    def build_vocab(self, texts, min_freq):\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for word in self.tokenize(text):\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        idx = 2\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    def encode(self, text, max_len=MAX_SEQ_LEN):\n",
    "        tokens = self.tokenize(text)\n",
    "        ids = [self.word2idx.get(t, 1) for t in tokens]\n",
    "        ids = ids[-max_len:] if len(ids) > max_len else [0]*(max_len - len(ids)) + ids\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform: image = self.transform(image)\n",
    "        input_ids = self.tokenizer.encode(str(row[\"Captions\"]))\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, input_ids, label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channel, channel // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channel // reduction, channel),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        w = self.fc(x).unsqueeze(-1).unsqueeze(-1)\n",
    "        return x * w\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            SEBlock(128),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x).flatten(1)\n",
    "\n",
    "\n",
    "# ====================== LSTM Encoder ======================\n",
    "class GRUTextEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "        num_embeddings, emb_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, HIDDEN_DIM, batch_first=True, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        self.norm = nn.LayerNorm(HIDDEN_DIM * 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.dropout(self.embedding(input_ids))\n",
    "        _, hn = self.gru(emb)\n",
    "        feat = torch.cat((hn[-2], hn[-1]), dim=1)\n",
    "        return self.norm(feat)\n",
    "\n",
    "\n",
    "# ====================== Final Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "        self.vision_model = CustomCNN()\n",
    "        self.text_model = GRUTextEncoder(embedding_matrix)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 + HIDDEN_DIM*2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, NUM_CLASSES)\n",
    "        )\n",
    "    def forward(self, image, input_ids):\n",
    "        vis_feat = self.vision_model(image)\n",
    "        txt_feat = self.text_model(input_ids)\n",
    "        x = torch.cat([vis_feat, txt_feat], dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Transforms ======================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    transforms.RandomAffine(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# ====================== Load Data ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "texts = df[\"Captions\"].astype(str).tolist()\n",
    "tokenizer = SimpleTokenizer(texts)\n",
    "embedding_matrix = load_glove_embeddings(glove_path, tokenizer.word2idx, EMBEDDING_DIM)\n",
    "\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\", transform=train_transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "train_set.dataset.transform = train_transform\n",
    "val_set.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================== Train ======================\n",
    "model = VisionTextClassifier(embedding_matrix).to(device)\n",
    "cls_weights = compute_class_weight('balanced', classes=np.unique(df[\"Label_Sentiment\"].str.lower()), y=df[\"Label_Sentiment\"].str.lower())\n",
    "weights_tensor = torch.tensor([cls_weights[LABEL_MAP[k]] for k in LABEL_MAP], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\")\n",
    "    for img, ids, lbl in loop:\n",
    "        img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img, ids)\n",
    "        loss = criterion(out, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets, val_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for img, ids, lbl in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "            img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "            out = model(img, ids)\n",
    "            val_loss += criterion(out, lbl).item()\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "    val_accs.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={val_losses[-1]:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "# ====================== Plot ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random, numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "# ====================== Load GloVe ======================\n",
    "def load_glove_embeddings(glove_path, word2idx, embed_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if word in word2idx:\n",
    "                vector = torch.tensor([float(v) for v in values[1:]], dtype=torch.float)\n",
    "                embeddings_index[word] = vector\n",
    "    embedding_matrix = torch.randn(len(word2idx), embed_dim) * 0.05\n",
    "    embedding_matrix[0] = torch.zeros(embed_dim)\n",
    "    for word, idx in word2idx.items():\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "    return embedding_matrix\n",
    "\n",
    "# ====================== Tokenizer ======================\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts, min_freq=1):\n",
    "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.build_vocab(texts, min_freq)\n",
    "\n",
    "    def build_vocab(self, texts, min_freq):\n",
    "        word_freq = {}\n",
    "        for text in texts:\n",
    "            for word in self.tokenize(text):\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        idx = 2\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "    def encode(self, text, max_len=MAX_SEQ_LEN):\n",
    "        tokens = self.tokenize(text)\n",
    "        ids = [self.word2idx.get(t, 1) for t in tokens]\n",
    "        ids = ids[-max_len:] if len(ids) > max_len else [0]*(max_len - len(ids)) + ids\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform: image = self.transform(image)\n",
    "        input_ids = self.tokenizer.encode(str(row[\"Captions\"]))\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, input_ids, label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x).flatten(1)\n",
    "\n",
    "# ====================== LSTM Encoder ======================\n",
    "class LSTMTextEncoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "        num_embeddings, emb_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, HIDDEN_DIM, batch_first=True, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.dropout(self.embedding(input_ids))\n",
    "        output, (hn, cn) = self.lstm(emb)\n",
    "        feat = torch.cat((hn[-2], hn[-1]), dim=1)\n",
    "        return feat\n",
    "\n",
    "# ====================== Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super().__init__()\n",
    "        self.vision_model = CustomCNN()\n",
    "        self.text_model = LSTMTextEncoder(embedding_matrix)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 + HIDDEN_DIM*2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids):\n",
    "        vis_feat = self.vision_model(image)\n",
    "        txt_feat = self.text_model(input_ids)\n",
    "        x = torch.cat([vis_feat, txt_feat], dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Transforms ======================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# ====================== Load Dataset ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "texts = df[\"Captions\"].astype(str).tolist()\n",
    "tokenizer = SimpleTokenizer(texts)\n",
    "embedding_matrix = load_glove_embeddings(\"glove/glove.6B.100d.txt\", tokenizer.word2idx, EMBEDDING_DIM)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(df[\"Label_Sentiment\"].map(LABEL_MAP)), y=df[\"Label_Sentiment\"].map(LABEL_MAP))\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\", transform=train_transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "train_set.dataset.transform = train_transform\n",
    "val_set.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================== Training ======================\n",
    "model = VisionTextClassifier(embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\")\n",
    "    for img, ids, lbl in loop:\n",
    "        img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img, ids)\n",
    "        loss = criterion(out, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets, val_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for img, ids, lbl in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "            img, ids, lbl = img.to(device), ids.to(device), lbl.to(device)\n",
    "            out = model(img, ids)\n",
    "            val_loss += criterion(out, lbl).item()\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "    val_accs.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={val_losses[-1]:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "    print(classification_report(targets, preds, target_names=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)]))\n",
    "\n",
    "# ====================== Plots ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random, numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "MAX_SEQ_LEN = 128\n",
    "HIDDEN_DIM = 128\n",
    "IMAGE_DIM = 128\n",
    "TEXT_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        encoded = self.tokenizer(row[\"Captions\"], padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LEN, return_tensors=\"pt\")\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0), label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class CNNImageEncoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CNNImageEncoder, self).__init__()\n",
    "        from torchvision import models\n",
    "        base_model = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        avg_feat = self.avgpool(x).view(x.size(0), -1)\n",
    "        max_feat = self.maxpool(x).view(x.size(0), -1)\n",
    "        feat = torch.cat([avg_feat, max_feat], dim=1)\n",
    "        return self.fc(feat)\n",
    "\n",
    "# ====================== Text Encoder ======================\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.gru = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        x, _ = self.lstm(x)\n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Multimodal Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, image_dim, text_dim, hidden_dim, num_classes):\n",
    "        super(VisionTextClassifier, self).__init__()\n",
    "        self.image_encoder = CNNImageEncoder(image_dim)\n",
    "        self.text_encoder = TextEncoder(hidden_dim, text_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(image_dim + text_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        img_feat = self.image_encoder(image)\n",
    "        txt_feat = self.text_encoder(input_ids, attention_mask)\n",
    "        x = torch.cat((img_feat, txt_feat), dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ====================== Tokenizer and Dataset ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================== Training ======================\n",
    "model = VisionTextClassifier(IMAGE_DIM, TEXT_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(df[\"Label_Sentiment\"].map(LABEL_MAP)), y=df[\"Label_Sentiment\"].map(LABEL_MAP))\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img, ids, mask, lbl in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\"):\n",
    "        img, ids, mask, lbl = img.to(device), ids.to(device), mask.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img, ids, mask)\n",
    "        loss = criterion(output, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets, val_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for img, ids, mask, lbl in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "            img, ids, mask, lbl = img.to(device), ids.to(device), mask.to(device), lbl.to(device)\n",
    "            output = model(img, ids, mask)\n",
    "            val_loss += criterion(output, lbl).item()\n",
    "            preds.extend(output.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.cpu().numpy())\n",
    "    scheduler.step()\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accs.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={val_losses[-1]:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "    print(classification_report(targets, preds, target_names=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)]))\n",
    "\n",
    "# ====================== Plots ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random, numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "MAX_SEQ_LEN = 128\n",
    "HIDDEN_DIM = 128\n",
    "IMAGE_DIM = 128\n",
    "TEXT_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        encoded = self.tokenizer(row[\"Captions\"], padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LEN, return_tensors=\"pt\")\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0), label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class CNNImageEncoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CNNImageEncoder, self).__init__()\n",
    "        from torchvision import models\n",
    "        base_model = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(512 * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        avg_feat = self.avgpool(x).view(x.size(0), -1)\n",
    "        max_feat = self.maxpool(x).view(x.size(0), -1)\n",
    "        feat = torch.cat([avg_feat, max_feat], dim=1)\n",
    "        feat = self.dropout(feat)\n",
    "        return self.fc(feat)\n",
    "\n",
    "# ====================== Text Encoder ======================\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.gru = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        x, _ = self.lstm(x)\n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Multimodal Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, image_dim, text_dim, hidden_dim, num_classes):\n",
    "        super(VisionTextClassifier, self).__init__()\n",
    "        self.image_encoder = CNNImageEncoder(image_dim)\n",
    "        self.text_encoder = TextEncoder(hidden_dim, text_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(image_dim + text_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        img_feat = self.image_encoder(image)\n",
    "        txt_feat = self.text_encoder(input_ids, attention_mask)\n",
    "        x = torch.cat((img_feat, txt_feat), dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ====================== Tokenizer and Dataset ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================== Model, Loss, Optimizer ======================\n",
    "model = VisionTextClassifier(IMAGE_DIM, TEXT_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(df[\"Label_Sentiment\"].map(LABEL_MAP)), y=df[\"Label_Sentiment\"].map(LABEL_MAP))\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
    "\n",
    "# Optional: Use focal loss\n",
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, alpha=1, gamma=2, weight=None):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.gamma = gamma\n",
    "#         self.weight = weight\n",
    "#     def forward(self, input, target):\n",
    "#         ce_loss = nn.CrossEntropyLoss(weight=self.weight, reduction='none')(input, target)\n",
    "#         pt = torch.exp(-ce_loss)\n",
    "#         return (self.alpha * (1-pt)**self.gamma * ce_loss).mean()\n",
    "# criterion = FocalLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# ====================== Training ======================\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img, ids, mask, lbl in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\"):\n",
    "        img, ids, mask, lbl = img.to(device), ids.to(device), mask.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img, ids, mask)\n",
    "        loss = criterion(output, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets, val_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for img, ids, mask, lbl in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "            img, ids, mask, lbl = img.to(device), ids.to(device), mask.to(device), lbl.to(device)\n",
    "            output = model(img, ids, mask)\n",
    "            val_loss += criterion(output, lbl).item()\n",
    "            preds.extend(output.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.cpu().numpy())\n",
    "    scheduler.step()\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accs.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={val_losses[-1]:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "    print(classification_report(targets, preds, target_names=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)]))\n",
    "\n",
    "# ====================== Plots ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random, numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "MAX_SEQ_LEN = 128\n",
    "HIDDEN_DIM = 128\n",
    "IMAGE_DIM = 128\n",
    "TEXT_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        encoded = self.tokenizer(row[\"Captions\"], padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LEN, return_tensors=\"pt\")\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0), label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class CNNImageEncoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CNNImageEncoder, self).__init__()\n",
    "        from torchvision import models\n",
    "        base_model = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(512 * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        avg_feat = self.avgpool(x).view(x.size(0), -1)\n",
    "        max_feat = self.maxpool(x).view(x.size(0), -1)\n",
    "        feat = torch.cat([avg_feat, max_feat], dim=1)\n",
    "        feat = self.dropout(feat)\n",
    "        return self.fc(feat)\n",
    "\n",
    "# ====================== Text Encoder ======================\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.gru = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        x, _ = self.lstm(x)\n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Multimodal Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, image_dim, text_dim, hidden_dim, num_classes):\n",
    "        super(VisionTextClassifier, self).__init__()\n",
    "        self.image_encoder = CNNImageEncoder(image_dim)\n",
    "        self.text_encoder = TextEncoder(hidden_dim, text_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(image_dim + text_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        img_feat = self.image_encoder(image)\n",
    "        txt_feat = self.text_encoder(input_ids, attention_mask)\n",
    "        x = torch.cat((img_feat, txt_feat), dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ====================== Tokenizer and Dataset ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====================== Model, Loss, Optimizer ======================\n",
    "model = VisionTextClassifier(IMAGE_DIM, TEXT_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(df[\"Label_Sentiment\"].map(LABEL_MAP)), y=df[\"Label_Sentiment\"].map(LABEL_MAP))\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
    "\n",
    "# Optional: Use focal loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = nn.CrossEntropyLoss(weight=self.weight, reduction='none')(input, target)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        return (self.alpha * (1-pt)**self.gamma * ce_loss).mean()\n",
    "criterion = FocalLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# ====================== Training ======================\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img, ids, mask, lbl in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\"):\n",
    "        img, ids, mask, lbl = img.to(device), ids.to(device), mask.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img, ids, mask)\n",
    "        loss = criterion(output, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets, val_loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for img, ids, mask, lbl in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "            img, ids, mask, lbl = img.to(device), ids.to(device), mask.to(device), lbl.to(device)\n",
    "            output = model(img, ids, mask)\n",
    "            val_loss += criterion(output, lbl).item()\n",
    "            preds.extend(output.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.cpu().numpy())\n",
    "    scheduler.step()\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accs.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    print(f\"Epoch {epoch+1}: Val Loss={val_losses[-1]:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
    "    print(classification_report(targets, preds, target_names=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)]))\n",
    "\n",
    "# ====================== Plots ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ad6afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 1/23: 100%|| 274/274 [02:06<00:00,  2.17it/s]\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss=270.7755  Val Acc=0.1133  F1=0.0805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00      2730\n",
      "    positive       0.48      0.15      0.23      1348\n",
      "     neutral       0.07      1.00      0.14       291\n",
      "\n",
      "    accuracy                           0.11      4369\n",
      "   macro avg       0.19      0.38      0.12      4369\n",
      "weighted avg       0.15      0.11      0.08      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 2/23: 100%|| 274/274 [02:09<00:00,  2.12it/s]\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss=183.8527  Val Acc=0.3353  F1=0.1838\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00      2730\n",
      "    positive       0.35      0.89      0.50      1348\n",
      "     neutral       0.28      0.92      0.43       291\n",
      "\n",
      "    accuracy                           0.34      4369\n",
      "   macro avg       0.21      0.60      0.31      4369\n",
      "weighted avg       0.13      0.34      0.18      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 3/23: 100%|| 274/274 [02:05<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss=126.8774  Val Acc=0.3527  F1=0.1949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.00      0.00      2730\n",
      "    positive       0.34      0.94      0.50      1348\n",
      "     neutral       0.41      0.90      0.57       291\n",
      "\n",
      "    accuracy                           0.35      4369\n",
      "   macro avg       0.53      0.62      0.36      4369\n",
      "weighted avg       0.65      0.35      0.19      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 4/23: 100%|| 274/274 [02:08<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Train Loss=102.2520  Val Acc=0.3786  F1=0.2358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.03      0.06      2730\n",
      "    positive       0.36      0.96      0.52      1348\n",
      "     neutral       0.43      0.97      0.60       291\n",
      "\n",
      "    accuracy                           0.38      4369\n",
      "   macro avg       0.57      0.65      0.39      4369\n",
      "weighted avg       0.71      0.38      0.24      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 5/23: 100%|| 274/274 [02:05<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Train Loss=88.6548  Val Acc=0.4143  F1=0.3195\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.11      0.19      2730\n",
      "    positive       0.38      0.91      0.53      1348\n",
      "     neutral       0.37      1.00      0.54       291\n",
      "\n",
      "    accuracy                           0.41      4369\n",
      "   macro avg       0.55      0.67      0.42      4369\n",
      "weighted avg       0.70      0.41      0.32      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 6/23: 100%|| 274/274 [02:10<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: Train Loss=57.2152  Val Acc=0.4640  F1=0.3955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.17      0.29      2730\n",
      "    positive       0.38      0.94      0.54      1348\n",
      "     neutral       0.54      1.00      0.70       291\n",
      "\n",
      "    accuracy                           0.46      4369\n",
      "   macro avg       0.61      0.70      0.51      4369\n",
      "weighted avg       0.71      0.46      0.40      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 7/23: 100%|| 274/274 [02:09<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: Train Loss=49.0700  Val Acc=0.4601  F1=0.3835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.16      0.27      2730\n",
      "    positive       0.38      0.96      0.54      1348\n",
      "     neutral       0.59      1.00      0.74       291\n",
      "\n",
      "    accuracy                           0.46      4369\n",
      "   macro avg       0.63      0.71      0.52      4369\n",
      "weighted avg       0.74      0.46      0.38      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 8/23: 100%|| 274/274 [02:09<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: Train Loss=32.0982  Val Acc=0.5006  F1=0.4483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.22      0.35      2730\n",
      "    positive       0.39      0.96      0.56      1348\n",
      "     neutral       0.71      1.00      0.83       291\n",
      "\n",
      "    accuracy                           0.50      4369\n",
      "   macro avg       0.68      0.73      0.58      4369\n",
      "weighted avg       0.75      0.50      0.45      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 9/23: 100%|| 274/274 [02:08<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: Train Loss=36.8922  Val Acc=0.4685  F1=0.3968\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.16      0.27      2730\n",
      "    positive       0.37      0.99      0.54      1348\n",
      "     neutral       0.87      0.99      0.92       291\n",
      "\n",
      "    accuracy                           0.47      4369\n",
      "   macro avg       0.73      0.71      0.58      4369\n",
      "weighted avg       0.77      0.47      0.40      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 10/23: 100%|| 274/274 [02:09<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: Train Loss=25.5973  Val Acc=0.5615  F1=0.5352\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.31      0.47      2730\n",
      "    positive       0.42      0.97      0.59      1348\n",
      "     neutral       0.82      1.00      0.90       291\n",
      "\n",
      "    accuracy                           0.56      4369\n",
      "   macro avg       0.73      0.76      0.65      4369\n",
      "weighted avg       0.78      0.56      0.54      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 11/23: 100%|| 274/274 [02:07<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: Train Loss=26.4542  Val Acc=0.5491  F1=0.5170\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.30      0.45      2730\n",
      "    positive       0.42      0.96      0.58      1348\n",
      "     neutral       0.69      1.00      0.82       291\n",
      "\n",
      "    accuracy                           0.55      4369\n",
      "   macro avg       0.69      0.75      0.62      4369\n",
      "weighted avg       0.77      0.55      0.52      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 12/23: 100%|| 274/274 [02:07<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: Train Loss=18.4028  Val Acc=0.5937  F1=0.5761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.36      0.53      2730\n",
      "    positive       0.44      0.98      0.60      1348\n",
      "     neutral       0.85      1.00      0.92       291\n",
      "\n",
      "    accuracy                           0.59      4369\n",
      "   macro avg       0.76      0.78      0.68      4369\n",
      "weighted avg       0.80      0.59      0.58      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 13/23: 100%|| 274/274 [02:10<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: Train Loss=49.1201  Val Acc=0.5697  F1=0.5459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.32      0.48      2730\n",
      "    positive       0.42      0.99      0.59      1348\n",
      "     neutral       0.96      1.00      0.98       291\n",
      "\n",
      "    accuracy                           0.57      4369\n",
      "   macro avg       0.79      0.77      0.68      4369\n",
      "weighted avg       0.81      0.57      0.55      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 14/23: 100%|| 274/274 [02:06<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: Train Loss=36.0855  Val Acc=0.6402  F1=0.6340\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.43      0.60      2730\n",
      "    positive       0.46      0.99      0.63      1348\n",
      "     neutral       0.95      1.00      0.97       291\n",
      "\n",
      "    accuracy                           0.64      4369\n",
      "   macro avg       0.80      0.81      0.73      4369\n",
      "weighted avg       0.82      0.64      0.63      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 15/23: 100%|| 274/274 [02:07<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: Train Loss=15.7046  Val Acc=0.7189  F1=0.7216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.56      0.71      2730\n",
      "    positive       0.53      0.98      0.69      1348\n",
      "     neutral       0.90      1.00      0.95       291\n",
      "\n",
      "    accuracy                           0.72      4369\n",
      "   macro avg       0.81      0.85      0.78      4369\n",
      "weighted avg       0.84      0.72      0.72      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 16/23: 100%|| 274/274 [02:09<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: Train Loss=22.7906  Val Acc=0.7693  F1=0.7736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.64      0.78      2730\n",
      "    positive       0.58      0.98      0.73      1348\n",
      "     neutral       0.90      1.00      0.95       291\n",
      "\n",
      "    accuracy                           0.77      4369\n",
      "   macro avg       0.82      0.87      0.82      4369\n",
      "weighted avg       0.86      0.77      0.77      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 17/23: 100%|| 274/274 [02:10<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: Train Loss=46.0260  Val Acc=0.7388  F1=0.7428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.59      0.74      2730\n",
      "    positive       0.55      0.99      0.70      1348\n",
      "     neutral       0.95      1.00      0.97       291\n",
      "\n",
      "    accuracy                           0.74      4369\n",
      "   macro avg       0.83      0.86      0.80      4369\n",
      "weighted avg       0.85      0.74      0.74      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 18/23: 100%|| 274/274 [02:09<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: Train Loss=12.4894  Val Acc=0.8043  F1=0.8086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.69      0.82      2730\n",
      "    positive       0.62      0.99      0.76      1348\n",
      "     neutral       0.90      1.00      0.94       291\n",
      "\n",
      "    accuracy                           0.80      4369\n",
      "   macro avg       0.84      0.89      0.84      4369\n",
      "weighted avg       0.87      0.80      0.81      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 19/23: 100%|| 274/274 [02:09<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: Train Loss=42.0412  Val Acc=0.8185  F1=0.8229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.72      0.83      2730\n",
      "    positive       0.64      0.99      0.78      1348\n",
      "     neutral       0.93      1.00      0.96       291\n",
      "\n",
      "    accuracy                           0.82      4369\n",
      "   macro avg       0.85      0.90      0.86      4369\n",
      "weighted avg       0.88      0.82      0.82      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 20/23: 100%|| 274/274 [02:08<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: Train Loss=9.9951  Val Acc=0.8121  F1=0.8167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.70      0.82      2730\n",
      "    positive       0.63      0.99      0.77      1348\n",
      "     neutral       0.94      1.00      0.97       291\n",
      "\n",
      "    accuracy                           0.81      4369\n",
      "   macro avg       0.85      0.90      0.85      4369\n",
      "weighted avg       0.88      0.81      0.82      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 21/23: 100%|| 274/274 [02:08<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: Train Loss=10.1398  Val Acc=0.8608  F1=0.8636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.79      0.88      2730\n",
      "    positive       0.71      0.98      0.83      1348\n",
      "     neutral       0.86      1.00      0.92       291\n",
      "\n",
      "    accuracy                           0.86      4369\n",
      "   macro avg       0.85      0.92      0.88      4369\n",
      "weighted avg       0.90      0.86      0.86      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 22/23: 100%|| 274/274 [02:10<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: Train Loss=12.6003  Val Acc=0.8036  F1=0.8086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.69      0.81      2730\n",
      "    positive       0.61      1.00      0.76      1348\n",
      "     neutral       0.96      1.00      0.98       291\n",
      "\n",
      "    accuracy                           0.80      4369\n",
      "   macro avg       0.86      0.89      0.85      4369\n",
      "weighted avg       0.88      0.80      0.81      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 23/23: 100%|| 274/274 [02:08<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: Train Loss=35.3895  Val Acc=0.7942  F1=0.7994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.67      0.80      2730\n",
      "    positive       0.60      1.00      0.75      1348\n",
      "     neutral       0.98      1.00      0.99       291\n",
      "\n",
      "    accuracy                           0.79      4369\n",
      "   macro avg       0.86      0.89      0.85      4369\n",
      "weighted avg       0.87      0.79      0.80      4369\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random, numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 23\n",
    "MAX_SEQ_LEN = 128\n",
    "HIDDEN_DIM = 128\n",
    "IMAGE_DIM = 128\n",
    "TEXT_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        encoded = self.tokenizer(row[\"Captions\"], padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LEN, return_tensors=\"pt\")\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0), label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class CNNImageEncoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        from torchvision import models\n",
    "        base_model = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512 * 2, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        avg_feat = self.avgpool(x).view(x.size(0), -1)\n",
    "        max_feat = self.maxpool(x).view(x.size(0), -1)\n",
    "        feat = torch.cat([avg_feat, max_feat], dim=1)\n",
    "        return self.fc(feat)\n",
    "\n",
    "# ====================== Text Encoder ======================\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"csebuetnlp/banglabert\")\n",
    "        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.gru = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        x, _ = self.lstm(x)\n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, image_dim, text_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.image_encoder = CNNImageEncoder(image_dim)\n",
    "        self.text_encoder = TextEncoder(hidden_dim, text_dim)\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(image_dim + text_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(image_dim + text_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        img_feat = self.image_encoder(image)\n",
    "        txt_feat = self.text_encoder(input_ids, attention_mask)\n",
    "        combined = torch.cat((img_feat, txt_feat), dim=1)\n",
    "        weight = self.attn(combined)\n",
    "        fusion = weight * img_feat + (1 - weight) * txt_feat\n",
    "        full_feat = torch.cat([img_feat, txt_feat], dim=1)\n",
    "        return self.classifier(full_feat)\n",
    "\n",
    "# ====================== Dataset & Sampler ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"csebuetnlp/banglabert\")\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\")\n",
    "\n",
    "# Balanced sampler\n",
    "labels = df[\"Label_Sentiment\"].map(LABEL_MAP).values\n",
    "class_counts = np.bincount(labels)\n",
    "weights = 1. / class_counts[labels]\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "\n",
    "# ====================== Model, Loss, Optimizer ======================\n",
    "model = VisionTextClassifier(IMAGE_DIM, TEXT_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = nn.CrossEntropyLoss(weight=self.weight, reduction='none')(input, target)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        return (self.alpha * (1-pt)**self.gamma * ce_loss).mean()\n",
    "\n",
    "criterion = FocalLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# ====================== Training ======================\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img, ids, mask, lbl in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        img, ids, mask, lbl = img.to(device), ids.to(device), mask.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img, ids, mask)\n",
    "        loss = criterion(out, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for img, ids, mask, lbl in DataLoader(dataset, batch_size=BATCH_SIZE):\n",
    "            img, ids, mask = img.to(device), ids.to(device), mask.to(device)\n",
    "            out = model(img, ids, mask)\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.numpy())\n",
    "            # val_loss += criterion(output, lbl).item()\n",
    "\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average='weighted')\n",
    "    print(f\"\\nEpoch {epoch+1}: Train Loss={total_loss:.4f}  Val Acc={acc:.4f}  F1={f1:.4f}\")\n",
    "    print(classification_report(targets, preds, target_names=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)]))\n",
    "    scheduler.step()\n",
    "    # val_losses.append(val_loss / len(val_loader))\n",
    "    # val_accs.append(acc)\n",
    "    # val_f1s.append(f1)\n",
    "    # ====================== Plots ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2575c7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 1/23: 100%|| 274/274 [01:48<00:00,  2.53it/s]\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss=264.9633  Val Acc=0.2513  F1=0.1601\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00      2730\n",
      "    positive       0.38      0.62      0.47      1348\n",
      "     neutral       0.12      0.90      0.21       291\n",
      "\n",
      "    accuracy                           0.25      4369\n",
      "   macro avg       0.17      0.51      0.23      4369\n",
      "weighted avg       0.13      0.25      0.16      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 2/23: 100%|| 274/274 [02:01<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss=176.2451  Val Acc=0.3117  F1=0.2111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.03      0.05      2730\n",
      "    positive       0.40      0.74      0.52      1348\n",
      "     neutral       0.16      0.98      0.28       291\n",
      "\n",
      "    accuracy                           0.31      4369\n",
      "   macro avg       0.47      0.58      0.28      4369\n",
      "weighted avg       0.66      0.31      0.21      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 3/23: 100%|| 274/274 [02:02<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss=127.2297  Val Acc=0.3628  F1=0.2497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.05      0.10      2730\n",
      "    positive       0.38      0.86      0.53      1348\n",
      "     neutral       0.25      1.00      0.40       291\n",
      "\n",
      "    accuracy                           0.36      4369\n",
      "   macro avg       0.49      0.64      0.34      4369\n",
      "weighted avg       0.66      0.36      0.25      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 4/23: 100%|| 274/274 [02:05<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Train Loss=102.6566  Val Acc=0.3658  F1=0.2260\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.03      0.05      2730\n",
      "    positive       0.36      0.92      0.52      1348\n",
      "     neutral       0.34      0.98      0.51       291\n",
      "\n",
      "    accuracy                           0.37      4369\n",
      "   macro avg       0.52      0.64      0.36      4369\n",
      "weighted avg       0.67      0.37      0.23      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 5/23: 100%|| 274/274 [02:01<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Train Loss=89.8111  Val Acc=0.3999  F1=0.2799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.07      0.12      2730\n",
      "    positive       0.37      0.95      0.53      1348\n",
      "     neutral       0.43      0.98      0.60       291\n",
      "\n",
      "    accuracy                           0.40      4369\n",
      "   macro avg       0.56      0.67      0.42      4369\n",
      "weighted avg       0.70      0.40      0.28      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 6/23: 100%|| 274/274 [02:02<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: Train Loss=69.7249  Val Acc=0.3978  F1=0.2660\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      0.04      0.08      2730\n",
      "    positive       0.35      0.99      0.51      1348\n",
      "     neutral       0.73      0.99      0.84       291\n",
      "\n",
      "    accuracy                           0.40      4369\n",
      "   macro avg       0.68      0.67      0.48      4369\n",
      "weighted avg       0.76      0.40      0.27      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 7/23: 100%|| 274/274 [01:50<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: Train Loss=44.5512  Val Acc=0.4404  F1=0.3466\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      0.11      0.20      2730\n",
      "    positive       0.36      0.99      0.53      1348\n",
      "     neutral       0.80      1.00      0.89       291\n",
      "\n",
      "    accuracy                           0.44      4369\n",
      "   macro avg       0.71      0.70      0.54      4369\n",
      "weighted avg       0.77      0.44      0.35      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 8/23: 100%|| 274/274 [02:02<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: Train Loss=38.1925  Val Acc=0.5006  F1=0.4471\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.96      0.21      0.35      2730\n",
      "    positive       0.39      0.97      0.56      1348\n",
      "     neutral       0.76      1.00      0.86       291\n",
      "\n",
      "    accuracy                           0.50      4369\n",
      "   macro avg       0.70      0.73      0.59      4369\n",
      "weighted avg       0.77      0.50      0.45      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 9/23: 100%|| 274/274 [02:01<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: Train Loss=54.7776  Val Acc=0.4351  F1=0.3374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.10      0.18      2730\n",
      "    positive       0.36      0.99      0.52      1348\n",
      "     neutral       0.86      1.00      0.93       291\n",
      "\n",
      "    accuracy                           0.44      4369\n",
      "   macro avg       0.73      0.70      0.54      4369\n",
      "weighted avg       0.78      0.44      0.34      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 10/23: 100%|| 274/274 [01:54<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: Train Loss=53.1514  Val Acc=0.4701  F1=0.3982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.16      0.27      2730\n",
      "    positive       0.37      0.99      0.54      1348\n",
      "     neutral       0.90      0.99      0.94       291\n",
      "\n",
      "    accuracy                           0.47      4369\n",
      "   macro avg       0.75      0.71      0.58      4369\n",
      "weighted avg       0.79      0.47      0.40      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 11/23: 100%|| 274/274 [02:01<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: Train Loss=23.3071  Val Acc=0.5800  F1=0.5579\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.98      0.34      0.50      2730\n",
      "    positive       0.43      0.99      0.60      1348\n",
      "     neutral       0.85      1.00      0.92       291\n",
      "\n",
      "    accuracy                           0.58      4369\n",
      "   macro avg       0.75      0.77      0.67      4369\n",
      "weighted avg       0.80      0.58      0.56      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 12/23: 100%|| 274/274 [01:52<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: Train Loss=40.9540  Val Acc=0.5969  F1=0.5794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.36      0.53      2730\n",
      "    positive       0.44      0.99      0.61      1348\n",
      "     neutral       0.86      1.00      0.93       291\n",
      "\n",
      "    accuracy                           0.60      4369\n",
      "   macro avg       0.76      0.78      0.69      4369\n",
      "weighted avg       0.81      0.60      0.58      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 13/23: 100%|| 274/274 [02:02<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: Train Loss=40.1573  Val Acc=0.4994  F1=0.4431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.20      0.33      2730\n",
      "    positive       0.38      1.00      0.55      1348\n",
      "     neutral       0.92      1.00      0.96       291\n",
      "\n",
      "    accuracy                           0.50      4369\n",
      "   macro avg       0.77      0.73      0.62      4369\n",
      "weighted avg       0.80      0.50      0.44      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 14/23: 100%|| 274/274 [02:01<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: Train Loss=16.2223  Val Acc=0.5555  F1=0.5253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.29      0.45      2730\n",
      "    positive       0.41      1.00      0.58      1348\n",
      "     neutral       0.94      1.00      0.97       291\n",
      "\n",
      "    accuracy                           0.56      4369\n",
      "   macro avg       0.78      0.76      0.67      4369\n",
      "weighted avg       0.81      0.56      0.53      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 15/23: 100%|| 274/274 [02:01<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: Train Loss=16.0169  Val Acc=0.6567  F1=0.6519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.45      0.62      2730\n",
      "    positive       0.48      0.99      0.65      1348\n",
      "     neutral       0.92      1.00      0.96       291\n",
      "\n",
      "    accuracy                           0.66      4369\n",
      "   macro avg       0.80      0.82      0.74      4369\n",
      "weighted avg       0.83      0.66      0.65      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 16/23: 100%|| 274/274 [02:03<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: Train Loss=42.2402  Val Acc=0.6402  F1=0.6338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.43      0.60      2730\n",
      "    positive       0.46      0.99      0.63      1348\n",
      "     neutral       0.97      1.00      0.99       291\n",
      "\n",
      "    accuracy                           0.64      4369\n",
      "   macro avg       0.81      0.81      0.74      4369\n",
      "weighted avg       0.83      0.64      0.63      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 17/23: 100%|| 274/274 [02:01<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: Train Loss=43.4550  Val Acc=0.6317  F1=0.6232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.41      0.58      2730\n",
      "    positive       0.46      1.00      0.63      1348\n",
      "     neutral       0.97      1.00      0.98       291\n",
      "\n",
      "    accuracy                           0.63      4369\n",
      "   macro avg       0.81      0.80      0.73      4369\n",
      "weighted avg       0.83      0.63      0.62      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 18/23: 100%|| 274/274 [02:02<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: Train Loss=10.8656  Val Acc=0.7542  F1=0.7586\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.61      0.76      2730\n",
      "    positive       0.56      0.99      0.72      1348\n",
      "     neutral       0.95      1.00      0.98       291\n",
      "\n",
      "    accuracy                           0.75      4369\n",
      "   macro avg       0.84      0.87      0.82      4369\n",
      "weighted avg       0.86      0.75      0.76      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 19/23: 100%|| 274/274 [02:03<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: Train Loss=10.1116  Val Acc=0.7741  F1=0.7788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.64      0.78      2730\n",
      "    positive       0.58      0.99      0.73      1348\n",
      "     neutral       0.94      1.00      0.97       291\n",
      "\n",
      "    accuracy                           0.77      4369\n",
      "   macro avg       0.84      0.88      0.83      4369\n",
      "weighted avg       0.86      0.77      0.78      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 20/23: 100%|| 274/274 [02:02<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: Train Loss=40.8677  Val Acc=0.7530  F1=0.7575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.61      0.75      2730\n",
      "    positive       0.56      1.00      0.72      1348\n",
      "     neutral       0.96      1.00      0.98       291\n",
      "\n",
      "    accuracy                           0.75      4369\n",
      "   macro avg       0.84      0.87      0.82      4369\n",
      "weighted avg       0.86      0.75      0.76      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 21/23: 100%|| 274/274 [02:02<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: Train Loss=10.4620  Val Acc=0.8162  F1=0.8209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      0.71      0.83      2730\n",
      "    positive       0.63      0.99      0.77      1348\n",
      "     neutral       0.95      1.00      0.98       291\n",
      "\n",
      "    accuracy                           0.82      4369\n",
      "   macro avg       0.86      0.90      0.86      4369\n",
      "weighted avg       0.88      0.82      0.82      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 22/23: 100%|| 274/274 [02:04<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: Train Loss=9.5289  Val Acc=0.8306  F1=0.8350\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.73      0.84      2730\n",
      "    positive       0.65      1.00      0.79      1348\n",
      "     neutral       0.95      1.00      0.97       291\n",
      "\n",
      "    accuracy                           0.83      4369\n",
      "   macro avg       0.87      0.91      0.87      4369\n",
      "weighted avg       0.89      0.83      0.83      4369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/23:   0%|          | 0/274 [00:00<?, ?it/s]c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Epoch 23/23: 100%|| 274/274 [02:01<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: Train Loss=7.6501  Val Acc=0.8581  F1=0.8616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.78      0.87      2730\n",
      "    positive       0.69      0.99      0.82      1348\n",
      "     neutral       0.93      1.00      0.97       291\n",
      "\n",
      "    accuracy                           0.86      4369\n",
      "   macro avg       0.87      0.92      0.88      4369\n",
      "weighted avg       0.90      0.86      0.86      4369\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================== Imports ======================\n",
    "import os, re, torch, random, numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# ====================== Constants ======================\n",
    "LABEL_MAP = {'positive': 1, 'neutral': 2, 'negative': 0}\n",
    "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 23\n",
    "MAX_SEQ_LEN = 128\n",
    "HIDDEN_DIM = 128\n",
    "IMAGE_DIM = 128\n",
    "TEXT_DIM = 128\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ====================== Seed Fix ======================\n",
    "def seed_everything(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "\n",
    "# ====================== Dataset ======================\n",
    "class VisionTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_folder, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        encoded = self.tokenizer(row[\"Captions\"], padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LEN, return_tensors=\"pt\")\n",
    "        label = LABEL_MAP[row[\"Label_Sentiment\"].strip().lower()]\n",
    "        return image, encoded[\"input_ids\"].squeeze(0), encoded[\"attention_mask\"].squeeze(0), label\n",
    "\n",
    "# ====================== CNN Encoder ======================\n",
    "class CNNImageEncoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        from torchvision import models\n",
    "        base_model = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512 * 2, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        avg_feat = self.avgpool(x).view(x.size(0), -1)\n",
    "        max_feat = self.maxpool(x).view(x.size(0), -1)\n",
    "        feat = torch.cat([avg_feat, max_feat], dim=1)\n",
    "        return self.fc(feat)\n",
    "\n",
    "# ====================== Text Encoder ======================\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.gru = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        x, _ = self.lstm(x)\n",
    "        x, _ = self.gru(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====================== Classifier ======================\n",
    "class VisionTextClassifier(nn.Module):\n",
    "    def __init__(self, image_dim, text_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.image_encoder = CNNImageEncoder(image_dim)\n",
    "        self.text_encoder = TextEncoder(hidden_dim, text_dim)\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(image_dim + text_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(image_dim + text_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        img_feat = self.image_encoder(image)\n",
    "        txt_feat = self.text_encoder(input_ids, attention_mask)\n",
    "        combined = torch.cat((img_feat, txt_feat), dim=1)\n",
    "        weight = self.attn(combined)\n",
    "        fusion = weight * img_feat + (1 - weight) * txt_feat\n",
    "        full_feat = torch.cat([img_feat, txt_feat], dim=1)\n",
    "        return self.classifier(full_feat)\n",
    "\n",
    "# ====================== Dataset & Sampler ======================\n",
    "df = pd.read_excel(\"Dataset/multi-sent.xlsx\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = VisionTextDataset(df, tokenizer, \"Dataset/Memes\")\n",
    "\n",
    "# Balanced sampler\n",
    "labels = df[\"Label_Sentiment\"].map(LABEL_MAP).values\n",
    "class_counts = np.bincount(labels)\n",
    "weights = 1. / class_counts[labels]\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "\n",
    "# ====================== Model, Loss, Optimizer ======================\n",
    "model = VisionTextClassifier(IMAGE_DIM, TEXT_DIM, HIDDEN_DIM, NUM_CLASSES).to(device)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = nn.CrossEntropyLoss(weight=self.weight, reduction='none')(input, target)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        return (self.alpha * (1-pt)**self.gamma * ce_loss).mean()\n",
    "\n",
    "criterion = FocalLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# ====================== Training ======================\n",
    "train_losses, val_losses, val_accs, val_f1s = [], [], [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img, ids, mask, lbl in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        img, ids, mask, lbl = img.to(device), ids.to(device), mask.to(device), lbl.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img, ids, mask)\n",
    "        loss = criterion(out, lbl)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for img, ids, mask, lbl in DataLoader(dataset, batch_size=BATCH_SIZE):\n",
    "            img, ids, mask = img.to(device), ids.to(device), mask.to(device)\n",
    "            out = model(img, ids, mask)\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "            targets.extend(lbl.numpy())\n",
    "            # val_loss += criterion(output, lbl).item()\n",
    "\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average='weighted')\n",
    "    print(f\"\\nEpoch {epoch+1}: Train Loss={total_loss:.4f}  Val Acc={acc:.4f}  F1={f1:.4f}\")\n",
    "    print(classification_report(targets, preds, target_names=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)]))\n",
    "    scheduler.step()\n",
    "    # val_losses.append(val_loss / len(val_loader))\n",
    "    # val_accs.append(acc)\n",
    "    # val_f1s.append(f1)\n",
    "    # ====================== Plots ======================\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "plt.figure()\n",
    "sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[INV_LABEL_MAP[i] for i in range(NUM_CLASSES)])\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"plots/confusion_matrix.png\"); plt.show()\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\"); plt.legend(); plt.grid(); plt.savefig(\"plots/loss.png\"); plt.show()\n",
    "\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.plot(val_f1s, label=\"Val F1\")\n",
    "plt.title(\"Accuracy & F1\"); plt.legend(); plt.grid(); plt.savefig(\"plots/acc_f1.png\"); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
